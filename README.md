# ğŸ•¸ï¸ Projekt semestralny - Web Scraping
> âš ï¸ **Uwaga: Scraper jest dostosowany do strony â€Books to Scrapeâ€ (https://books.toscrape.com).**  
> Scraping danych ze stron bez wyraÅºnej zgody wÅ‚aÅ›ciciela jest nieetyczny i moÅ¼e naruszaÄ‡ regulamny serwisu lub prawa autorskie.  

## ğŸ‘¤Autorzy
- Kacper WoszczyÅ‚o - 21324
- MichaÅ‚ Lepak - 21255 
- Grupa: 1

## ğŸ“„ Cel projektu
1. Aplikacja pobiera, selekcjonuje i skÅ‚aduje  wybranedane o narzuconym profilu z witryn internetowych.
2. Profil danych jest ustalony przez realizujÄ…cego projekt. Profil danych powinien obejmowaÄ‡ min. 4 grupy, np. adresy email, adresy korespondencyjne, schemat organizacyjny itp.
3. Program wykorzystuje wielowÄ…tkowoÅ›Ä‡/wieloprocesowoÅ›Ä‡. Silnik naleÅ¼y zrealizowaÄ‡ we wÅ‚asnym zakresie wykorzystujÄ…c: multiprocessing i asyncio. Przetwarzanie ma byÄ‡ wieloprocesowe, najlepiej z moÅ¼liwoÅ›ciÄ… skalowania na rdzenie procesora, dalej na komputery, dalej na klastry itp.
4. Do parsowania kontentu naleÅ¼y uÅ¼yÄ‡ beautifulsoup.
5. Dane majÄ… byÄ‡ zapisywane w BD, np. MongoDB
6. Program ma posiadaÄ‡ interfejs graficzny zrealizowany w Python (Flask lub Django)
7. Docelowo aplikacja ma byÄ‡ rozproszona na min 3 moduÅ‚y: interfejs (1 lub wiÄ™cej kontenerÃ³w), silnik (1 kontener), BD (1 kontener). SposÃ³b ulokowania naleÅ¼y opracowaÄ‡ we wÅ‚asnym zakresie i potrafiÄ‡ uzasadniÄ‡ wybory.
8. Oprogramowanie moÅ¼e byÄ‡ zrealizowane w grupie 1 lub 2 osobowej.
9. Projekt uznaje siÄ™ za zÅ‚oÅ¼ony, jeÅ¼eli w wyznaczonym terminie zostanie opublikowany szczegÃ³Å‚owy raport z dowiÄ…zaniem do repozytorium kodu (github) oraz zostanie zademonstrowany prowadzÄ…cemu na ostatnich zajÄ™ciach laboratoryjnych.

## ğŸ“Œ Opis projektu
Niniejsza aplikacja to rozproszony system do **automatycznego pobierania, selekcjonowania i przechowywania danych** z witryn internetowych, zgodnie z uprzednio zdefiniowanym **profilem danych**. Celem projektu jest stworzenie skalowalnego i modularnego narzÄ™dzia do web scrapingu, ktÃ³re moÅ¼e funkcjonowaÄ‡ w Å›rodowisku wieloprocesorowym i rozproszonym.

## ğŸ“‹ Zakres funkcjonalnoÅ›ci

â¡ï¸ **Scrapowanie stron poprzez podanie linku**  
   - UÅ¼ytkownik wkleja URL pierwszej strony (np. `https://books.toscrape.com/catalogue/page-1.html`),  
   - Scraper Master automatycznie odkrywa kolejne strony paginacji,  
   - Informacje szczegÃ³Å‚owe (tytuÅ‚, cena, dostÄ™pnoÅ›Ä‡, kategoria, obrazek) sÄ… pobierane asynchronicznie i zapisywane w Redis.

â¡ï¸ **Sortowanie wedÅ‚ug potrzeb uÅ¼ytkownika**  
   - DostÄ™pne tryby sortowania:  
     - Nazwa rosnÄ…co (A â†’ Z)  
     - Nazwa malejÄ…co (Z â†’ A)  
     - Cena rosnÄ…co  
     - Cena malejÄ…co  
     - IloÅ›Ä‡ dostÄ™pna rosnÄ…co  
     - IloÅ›Ä‡ dostÄ™pna malejÄ…co  
   - Sortowanie odbywa siÄ™ po stronie serwera (Flask), na juÅ¼ zebranym zbiorze rekordÃ³w.

â¡ï¸ **PrzeglÄ…danie ponad 1000 ksiÄ…Å¼ek (i wiÄ™cej)**  
   - System zostaÅ‚ zoptymalizowany pod kÄ…tem duÅ¼ej liczby pozycji â€” Redis zapewnia szybki dostÄ™p do danych,  
   - Frontend wczytuje listÄ™ pozycji dynamicznie przy kaÅ¼dym odÅ›wieÅ¼eniu lub zmianie parametrÃ³w sortowania/filtrowania.

â¡ï¸ **WybÃ³r kategorii wedÅ‚ug preferencji uÅ¼ytkownika**  
   - Na stronie WWW jest dostÄ™pne pole tekstowe (filtr kategorii),  
   - UÅ¼ytkownik moÅ¼e wpisaÄ‡ fragment nazwy kategorii (np. â€travelâ€, â€scienceâ€),  
   - System wyÅ›wietli tylko te ksiÄ…Å¼ki, ktÃ³rych kategoria zawiera wpisanÄ… frazÄ™ (ignorujÄ…c wielkoÅ›Ä‡ liter).

â¡ï¸ **Wyszukiwanie ksiÄ…Å¼ek po nazwie**
- DostÄ™pna jest wyszukiwarka umoÅ¼liwiajÄ…ca filtrowanie ksiÄ…Å¼ek na podstawie tytuÅ‚u,
- Wyszukiwanie jest niewraÅ¼liwe na wielkoÅ›Ä‡ liter oraz dziaÅ‚a w czasie rzeczywistym.

â¡ï¸ **Automatyczny licznik ksiÄ…Å¼ek**
- Interfejs wyÅ›wietla caÅ‚kowitÄ… liczbÄ™ znalezionych ksiÄ…Å¼ek zgodnych z aktualnymi filtrami i wyszukiwaniem,
- Licznik aktualizuje siÄ™ automatycznie przy kaÅ¼dej zmianie parametrÃ³w (sortowanie, filtrowanie, wyszukiwanie).

## ğŸ“ Struktura repozytorium

```bash
Repozytorium:
â”‚â”€â”€â”€LICENSE
â”‚â”€â”€â”€PRiR_21324_21255.docx
â”‚â”€â”€â”€README.md
â”‚
â””â”€â”€â”€Projekt
    â”‚â”€â”€â”€komendy.txt
    â”‚
    â”œâ”€â”€â”€DB
    â”‚    â”œâ”€â”€Dockerfile
    â”‚    â””â”€â”€redis.conf
    â”‚
    â”œâ”€â”€â”€Interfejs
    â”‚   â”‚â”€â”€â”€app.py
    â”‚   â”‚â”€â”€â”€Dockerfile
    â”‚   â”‚â”€â”€â”€requirements.txt
    â”‚   â”‚
    â”‚   â”œâ”€â”€â”€static
    â”‚   â”‚    â””â”€â”€â”€style.css
    â”‚   â”‚
    â”‚   â””â”€â”€â”€templates
    â”‚        â””â”€â”€â”€index.html
    â”‚
    â”œâ”€â”€â”€k8s
    â”‚     â”œâ”€â”€â”€bd-deployment.yaml
    â”‚     â”œâ”€â”€â”€engine-deployment.yaml
    â”‚     â”œâ”€â”€â”€interface-deployment.yaml
    â”‚     â”œâ”€â”€â”€redis-configmap.yaml
    â”‚     â””â”€â”€â”€redis-pvc.yaml
    â”‚
    â””â”€â”€â”€Silnik
           â”œâ”€â”€â”€Dockerfile
           â”œâ”€â”€â”€requirements.txt
           â”œâ”€â”€â”€scraper_master.py
           â””â”€â”€â”€scraper_worker.py

```
---

## ğŸ§± Architektura systemu

Projekt zakÅ‚ada **modularnÄ…, kontenerowÄ… architekturÄ™**, ktÃ³ra skÅ‚ada siÄ™ z minimum **3 kontenerÃ³w**:

| ModuÅ‚                    | Opis                                                    |
|-------------------------------------------------------------|-----------------------------------------------------------|
| ğŸ§  Silnik              | Komponent odpowiedzialny za scraping, analizÄ™ i przetwarzanie danych; uruchamiany w kontenerze z Pythonem i multiprocessing |
| ğŸŒ Interfejs           | Serwer aplikacji webowej â€“ obsÅ‚uguje interakcjÄ™ z uÅ¼ytkownikiem, prezentuje dane i uruchamia zadania scrapujÄ…ce |
| ğŸ—„ï¸ Baza Danych (Redis)ã…¤ã…¤ã…¤| In-memory store â€“ przechowuje dane w strukturach klucz-wartoÅ›Ä‡; moÅ¼e peÅ‚niÄ‡ rolÄ™ bufora, kolejki zadaÅ„, cacheâ€™u lub tymczasowego magazynu wynikÃ³w  |

## ğŸ§ª Technologie

- * Python 3.x: * GÅ‚Ã³wny jÄ™zyk programowania uÅ¼ywany zarÃ³wno dla logiki silnika scrapujÄ…cego, jak i interfejsu webowego.
- * BeautifulSoup: * Biblioteka Pythona do parsowania HTML i XML, niezbÄ™dna dla silnika do ekstrakcji danych ze stron internetowych.
- * asyncio, multiprocessing: * ModuÅ‚y Pythona sÅ‚uÅ¼Ä…ce do zwiÄ™kszania wydajnoÅ›ci. asyncio odpowiada za asynchroniczne operacje I/O (np. pobieranie stron), a multiprocessing umoÅ¼liwia rÃ³wnolegÅ‚e przetwarzanie zadaÅ„, wykorzystujÄ…c wiele rdzeni procesora w silniku scrapujÄ…cym.
- **Flask:** Lekki framework webowy w Pythonie, uÅ¼yty do zbudowania interfejsu uÅ¼ytkownika aplikacji.
- **Redis:** Szybka baza danych dziaÅ‚ajÄ…ca w pamiÄ™ci, wykorzystywana do przechowywania danych ksiÄ…Å¼ek oraz jako kolejka do komunikacji miÄ™dzy interfejsem a silnikiem.
- **Docker:** Technologia do konteneryzacji aplikacji, umoÅ¼liwiajÄ…ca pakowanie kaÅ¼dego komponentu (bazy danych, interfejsu, silnika) w niezaleÅ¼ne, przenoÅ›ne kontenery.
- **Kubernetes (K8s):** System do automatyzacji wdraÅ¼ania, skalowania i zarzÄ…dzania skonteneryzowanymi aplikacjami, uÅ¼yty do orkiestracji wszystkich komponentÃ³w projektu.
- **HTML & CSS:** Standardowe technologie do tworzenia struktury (HTML) i stylizacji (CSS) interfejsu uÅ¼ytkownika dostÄ™pnego przez przeglÄ…darkÄ™.
- **json:** ModuÅ‚ Pythona do pracy z formatem danych JSON, wykorzystywany do serializacji i deserializacji danych przechowywanych w Redis.
- **os:** ModuÅ‚ Pythona do interakcji z systemem operacyjnym, czÄ™sto uÅ¼ywany do zarzÄ…dzania zmiennymi Å›rodowiskowymi (np. konfiguracja poÅ‚Ä…czenia z Redis).
- **time:** ModuÅ‚ Pythona zapewniajÄ…cy funkcje zwiÄ…zane z czasem, uÅ¼ywany w silniku do krÃ³tkich pauz w pÄ™tli gÅ‚Ã³wnej.

## ğŸš€ Uruchamianie aplikacji

```bash
# 1. WÅ‚Ä…czyÄ‡ Kubernetes w Docker Desktop (GUI).

# 2. ZbudowaÄ‡ obraz Redis:
cd Projekt/DB
docker build -t projekt-redis:latest .

# 3. ZbudowaÄ‡ obraz Silnika:
cd ../Silnik
docker build -t projekt-silnik:latest .

# 4. ZbudowaÄ‡ obraz Interfejs:
cd ../Interfejs
docker build -t projekt-interfejs:latest .

# 5. WdrÃ³Å¼yÄ‡ manifesty w Kubernetes:
cd ../k8s
kubectl apply -f redis-configmap.yaml
kubectl apply -f redis-pvc.yaml
kubectl apply -f bd-deployment.yaml
kubectl apply -f engine-deployment.yaml
kubectl apply -f interface-deployment.yaml

# 6. SprawdziÄ‡, czy pody dziaÅ‚ajÄ…:
kubectl get pods
kubectl get svc

# (opcjonalnie) 7. JeÅ¼eli chcesz wyczyÅ›ciÄ‡ Redis przed nowym scrapowaniem:
kubectl exec deployment/redis -- redis-cli FLUSHALL

# 8. WejÅ›Ä‡ na interfejs w przeglÄ…darce:
http://localhost:30000

# 9. PodaÄ‡ dowolny URLi kliknÄ…Ä‡ "Scrapuj".
https://books.toscrape.com/

# 10. ÅšledziÄ‡ logi silnika, aby zobaczyÄ‡ postÄ™p:
kubectl logs -f deployment/scraper-engine

# 11. Po zakoÅ„czeniu scrappingu odÅ›wieÅ¼yÄ‡ stronÄ™ w przeglÄ…darce i zobacz wyniki.
```
## ğŸ“ Podsumowanie
Zrealizowany projekt to nowoczesna, rozproszona aplikacja do web scrapingu, umoÅ¼liwiajÄ…ca automatyczne pobieranie danych z witryny Books to Scrape. System zostaÅ‚ zaprojektowany z naciskiem na modularnoÅ›Ä‡, wydajnoÅ›Ä‡ i skalowalnoÅ›Ä‡. DziÄ™ki wykorzystaniu kontenerÃ³w Docker oraz orkiestracji w Kubernetes, moÅ¼liwe jest Å‚atwe wdraÅ¼anie i skalowanie aplikacji.

GÅ‚Ã³wne cele projektu zostaÅ‚y zrealizowane:
- âœ… pobieranie danych z wielu stron
- âœ… asynchroniczne i wieloprocesowe przetwarzanie
- âœ… dynamiczny interfejs uÅ¼ytkownika
- âœ… sortowanie i filtrowanie danych w czasie rzeczywistym
- âœ… przechowywanie danych w pamiÄ™ci (Redis)
- âœ… peÅ‚na separacja moduÅ‚Ã³w w kontenerach

Projekt pokazuje praktyczne zastosowanie technologii takich jak Python, Flask, BeautifulSoup, Redis, Docker i Kubernetes w kontekÅ›cie przetwarzania danych i budowy aplikacji webowej. Aplikacja moÅ¼e byÄ‡ rozszerzana o kolejne ÅºrÃ³dÅ‚a danych, nowe funkcje analityczne oraz rozbudowany backend




---

## ğŸ“¬ Kontakt

W razie pytaÅ„:
- Email:   21324@student.ans-elblag.pl
- GitHub:  https://github.com/kacprow21324

---

**Licencja:** [`MIT`](./LICENSE.md)   
