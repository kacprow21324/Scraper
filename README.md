# ðŸ•¸ï¸ Distributed Web Data Harvester

## ðŸ“Œ Opis projektu

Niniejsza aplikacja to rozproszony system do **automatycznego pobierania, selekcjonowania i przechowywania danych** z witryn internetowych, zgodnie z uprzednio zdefiniowanym **profilem danych**. Celem projektu jest stworzenie skalowalnego i modularnego narzÄ™dzia do web scrapingu, ktÃ³re moÅ¼e funkcjonowaÄ‡ w Å›rodowisku wieloprocesorowym i rozproszonym.

> ðŸ§  **Ciekawostka od prof. CzyÅ¼aka:** Pierwszy znany system zbierajÄ…cy dane z internetu nazywaÅ‚ siÄ™ â€žWebCrawlerâ€ i powstaÅ‚ w 1994 roku. DziÅ› kaÅ¼dy z nas moÅ¼e mieÄ‡ wÅ‚asnego â€žcrawleraâ€ â€“ a jakÅ¼e, w Pythonie!

## ðŸ“‹ Zakres funkcjonalnoÅ›ci

- âœ… Automatyczne zbieranie danych z internetu na podstawie zadanego profilu
- âœ… ObsÅ‚uga minimum **4 grup danych**, np.:
  - Adresy email
  - Adresy korespondencyjne
  - Nazwy i struktury organizacyjne
  - Numery telefonÃ³w / linki / social media
- âœ… Wieloprocesowe przetwarzanie danych z wykorzystaniem:
  - `multiprocessing` (dystrybucja na rdzenie CPU)
  - `asyncio` (asynchroniczna obsÅ‚uga I/O)
- âœ… Parsowanie treÅ›ci przy pomocy `BeautifulSoup`
- âœ… Zapis danych w bazie danych **MongoDB**
- âœ… Interfejs graficzny (webowy) zrealizowany przy uÅ¼yciu:
  - `Flask` **lub** `Django`

## ðŸ§± Architektura systemu

Projekt zakÅ‚ada **modularnÄ…, kontenerowÄ… architekturÄ™**, ktÃ³ra skÅ‚ada siÄ™ z minimum **3 kontenerÃ³w**:

| ModuÅ‚             | Opis                                                           |
|------------------|----------------------------------------------------------------|
| ðŸ§  Silnik         | Komponent odpowiedzialny za scraping, analizÄ™ i przetwarzanie danych; uruchamiany w kontenerze z Pythonem i multiprocessing |
| ðŸŒ Interfejs      | Serwer aplikacji webowej â€“ obsÅ‚uguje interakcjÄ™ z uÅ¼ytkownikiem, prezentuje dane i uruchamia zadania scrapujÄ…ce |
| ðŸ—„ï¸ Baza Danych    | MongoDB w osobnym kontenerze â€“ przechowuje dane w elastycznym formacie dokumentowym (JSON-like) |

> ðŸ”§ SkalowalnoÅ›Ä‡:
> - poziom 1: rozbicie procesu na wiele CPU
> - poziom 2: uruchamianie silnikÃ³w scrapujÄ…cych na wielu maszynach
> - poziom 3: moÅ¼liwa integracja z klastrami obliczeniowymi (np. z uÅ¼yciem Celery + Redis)

## ðŸ§ª Technologie

- Python 3.x
- BeautifulSoup
- asyncio, multiprocessing
- Flask / Django
- MongoDB
- Docker, Docker Compose

## ðŸš€ Uruchamianie aplikacji

```bash
# 1. Klonowanie repozytorium
git clone https://github.com/twoj-uzytkownik/nazwa-repo.git
cd nazwa-repo

# 2. Uruchomienie kontenerÃ³w (Docker Compose)
docker-compose up --build
